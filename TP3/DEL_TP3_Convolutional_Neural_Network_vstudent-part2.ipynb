{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b3ea8",
   "metadata": {},
   "source": [
    "<font style=\"font-size: 3rem; color: darkviolet\"> Convolutional Neural Networks in TensorFlow - *part 2* </font>\n",
    "\n",
    "DEL - 2023/24 - TP3 (3h)\n",
    "\n",
    "*This assignment is inspired by the Deep Learning course on Coursera by Andrew Ng, Stanford University, for which we are thankful.*\n",
    "\n",
    "In this assignment, your task is to employ transfer learning on a pre-trained Convolutional Neural Network (CNN) to construct an Alpaca/Not Alpaca classifier. \n",
    "\n",
    "The pre-trained model, MobileNetV2, has already undergone training on the large ImageNet dataset, containing over 14 million images and 1000 classes. \n",
    "\n",
    "#### Main objectives:\n",
    "\n",
    "- Augment data\n",
    "- Adapt the pre-trained MobileNet model to the new data and task\n",
    "- Fine-tune the final layers of the classifier to further improve the accuracy of the model\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- [1 - Dataset Preparation and Data Augmentation](#1)\n",
    "- [2 - Using MobileNetV2 for Transfer Learning](#2)\n",
    "    - [2.1 - Fine-tuning the top layers](#3)\n",
    "    - [2.2 - Fine-tuning the model](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427728a",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## <font color='darkviolet'> 1 - Dataset Preparation and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dd393",
   "metadata": {},
   "source": [
    "<a name='ex-1.1'></a>\n",
    "### <font color='blue'> Exercise 1 - Dataset preparation\n",
    "    \n",
    "<font color='blue'> **1.1** <font color='black'> This exercise focuses on using the `ImageDataGenerator` module in TensorFlow to dynamically generate batches of tensor image data, incorporating real-time data augmentation directly from a specified directory. The `flow_from_directory()` method simplifies the creation of training and validation datasets. During this step, the images will be preprocessed, including resizing to a specified dimension.\n",
    "    \n",
    "For a comprehensive understanding of the implementation process, refer to the documentation available at:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator    \n",
    "    \n",
    "Note: Ensuring the use of the same `seed` guarantees consistency in the split between training and validation datasets.\n",
    "\n",
    "To enhance diversity in the training set and facilitate improved learning by the model, image augmentation involves random transformations such as flipping and rotating. Implement the following augmentations for the training set :\n",
    "* Rescaling: `rescale=1./255` normalizes pixel values to the range [0, 1].\n",
    "\n",
    "* Zooming: `rotation_range=0.2` randomly rotate images by a degree range.\n",
    "\n",
    "* Horizontal Flipping: `horizontal_flip=True` randomly flips images horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/dataset/\"\n",
    "batch_size = 32\n",
    "img_size = (128,128)\n",
    "validation_split = 0.2\n",
    "\n",
    "# Create data generator with data augmentation for training set\n",
    "#TODO\n",
    "\n",
    "# Create data generator without data augmentation for validation set\n",
    "#TODO\n",
    "\n",
    "# Create the training dataset and the validation dataset with an 80% split\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435f7ad",
   "metadata": {},
   "source": [
    "<font color='blue'> **1.2** <font color='black'> Explore your dataset: dimensions, size, labels, and plot a few images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d177b",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## <font color='darkviolet'> 2 - Using MobileNetV2 for Transfer Learning\n",
    "\n",
    "MobileNetV2 was trained on the ImageNet dataset for the task of image classification. The pretrained MobileNetV2 model serves as a starting point for transfer learning on the similar task of binary image classification.\n",
    "    \n",
    "Note: Since the pre-trained MobileNetV2 model was originally trained using normalization values in the range of [-1, 1], it's considered best practice to use the same normalization standard for your input data. You can achieve this by using the `tf.keras.applications.mobilenet_v2.preprocess_input` function.\n",
    "    \n",
    "Obtain the pre-trained MobileNetV2 model with weights learned from the ImageNet dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 with the top layer\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(128,128 3), include_top=True, weights='imagenet')\n",
    "\n",
    "# Get a summary of the architecture and parameters\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ec485",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### <font color='blue'> Exercise 2 - Fine-tuning on the top layers\n",
    "    \n",
    "<font color='blue'> **2.1** <font color='black'>  Adapting the pre-trained model for the targeted task of recognizing alpacas involves the following steps:\n",
    "\n",
    "1. Remove the top layer (classification layer) used for the original classification task;\n",
    "2. Add a new classification layer specifically designed for the task of recognizing alpacas;\n",
    "3. Freeze the base model ensuring the existing weights remain unchanged, allowing only training on the newly introduced layer(s); the frozen layers act as a feature extractor, and the training process focuses on fine-tuning the top layers for the specific task.\n",
    "    \n",
    "Consult the documentation as needed: https://keras.io/guides/transfer_learning/#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69464b8a",
   "metadata": {},
   "source": [
    "<font color='blue'> **2.2** <font color='black'> Train only the new classificayion layer. Plot and observe the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aec5",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### <font color='blue'> Exercise 3 - Fine-tuning the model\n",
    "\n",
    "<font color='blue'> **3.1** <font color='black'> To initiate this fine-tuning process, unfreeze the layers at the end of the network. Specify the layer from which fine-tuning begins and re-freeze all preceding layers. \n",
    "    \n",
    "Re-run the training for additional epochs and assess whether this fine-tuning improves the model's accuracy.    \n",
    "Feel free to experiment with the starting layer for fine-tuning, as the specific choice is somewhat arbitrary. \n",
    "    \n",
    "The crucial aspect is that the later layers capture finer details relevant to the specific task at hand, such as distinguishing alpacas based on distinctive features like pointy ears and hairy tails.\n",
    "    \n",
    "Employing a smaller learning rate ensures smaller adjustments to better accommodate the specifics of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37096525",
   "metadata": {},
   "source": [
    "<font color='blue'> **3.1** <font color='black'> Train the model. Plot and observe the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab79173",
   "metadata": {},
   "source": [
    "**Feel free to experiment with different strategies for data augmentation, fine-tuning, and custom layers.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
